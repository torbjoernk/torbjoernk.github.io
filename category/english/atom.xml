<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[English | Torbjörn Klatt]]></title>
  <link href="http://torbjoern-klatt.de/category/english/atom.xml" rel="self"/>
  <link href="http://torbjoern-klatt.de/"/>
  <updated>2018-02-14T21:15:34+01:00</updated>
  <id>http://torbjoern-klatt.de/</id>
  <author>
    <name><![CDATA[Torbjörn Klatt]]></name>
    <email><![CDATA[kontakt@torbjoern-klatt.de]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A new computer - Part 1]]></title>
    <link href="http://torbjoern-klatt.de/article/2015/06/12/new-computer-part-1/"/>
    <updated>2015-06-12T07:00:00+02:00</updated>
    <id>http://torbjoern-klatt.de/article/2015/06/12/new-computer-part-1</id>
    <content type="html"><![CDATA[<p>I don’t deny it, my old tower PC at home has seen better days.
It’s a <em>Core2Duo</em> system from 2007 and has served its duties.
Thus, I’m looking for a replacement for this system for at most 800€, which will be able to serve at
least for the next five years.</p>

<p>This is supposed to be a trilogy of articles.
In this first one I’ll describe my current about-to-retire system, the requirements for the new
and reasons for choosing the specific new components I’m about to order.<br />
The second part will cover the building of the new system while the third and last part will give
a resume of the new system including a few benchmarks as far as I’ll be able to conduct them.</p>

<!-- more -->

<h2 id="the-old-system">The Old System</h2>

<p>To start off, these are the specs of my current system:</p>

<table class="table">
  <thead>
    <tr>
      <th>Component</th>
      <th>Vendor</th>
      <th>Name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Case</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>PSU</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Motherboard</td>
      <td>Gigabyte</td>
      <td>P35-DS3R</td>
    </tr>
    <tr>
      <td>CPU</td>
      <td>Intel</td>
      <td>Core 2 Duo E6600 @ 2.4GHz</td>
    </tr>
    <tr>
      <td>Memory</td>
      <td>Kingston</td>
      <td>2x DDR2 1024MB PC2-6400<br />2x DDR2 512MB PC2-6400</td>
    </tr>
    <tr>
      <td>GPU</td>
      <td>nVidia</td>
      <td>GeForce 7600 GS (@400MHz, 512MB)</td>
    </tr>
    <tr>
      <td>HDD</td>
      <td>Western Digital</td>
      <td> </td>
    </tr>
    <tr>
      <td>TV Card</td>
      <td>Haupauge</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>Back in 2007, when I bought the system for way too much money, the system was rather silent and I
managed to sleep in the same room.
Today, even though I regularly clean the fans and radiators from all the dust, I’d rather put the
system in a cool basement I never have to enter.<br />
The overall performance of the system isn’t the best - no doubt - though it’s sufficient for
occasional surfing, home backing and recording TV shows.<br />
Currently, it’s only running <em>Windows 7</em>.</p>

<h2 id="requirements-for-a-new-system">Requirements for a New System</h2>

<ol>
  <li>
    <p><strong>Quiet</strong></p>

    <p>I love systems which you don’t hear when there are running, though I will not pay a fortune for
another half a decibel.</p>
  </li>
  <li>
    <p><strong>Performant for several years</strong></p>

    <p>The usual tasks as for the old system remain, but two main points will be added:
<em>gaming</em> and <em>code development</em>.<br />
I will not be playing the most recent 3D high-resolution games, but those released two years ago
should be running at highest resolution with decent FPS.<br />
The other main task I’ll be using this system for is programming mostly Python and C++ codes.
I might even be able to experiment with OpenCL, finally.
Thus, compiling for example the <em>Boost</em> libraries should not take half an hour.</p>
  </li>
  <li>
    <p><strong>Cheap</strong></p>

    <p>There is an upper bound of 800€ for the whole system - hopefully including shipping.</p>
  </li>
  <li>
    <p><strong>Power efficient</strong></p>

    <p>I really don’t see any point in wasting power, though here it’s the same as with the noise:
I will not pay a fortune for another percent efficiency.</p>
  </li>
  <li>
    <p><strong>Expandable</strong></p>

    <p>I’m planning on extending and upgrading the system over the years, thus there should be sufficient
head room for all components.</p>
  </li>
</ol>

<p>It will be running both Linux and Windows and I want a SSD as the main system drive with an
additional HDD for the gross of data.</p>

<p>From my point of view, this is an optimization problem in finding the best relation between the five
points above with a little more weight on the performance part.
However, I’m not going to derive a mathematical model for this, build a database of the various
components <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> and compute the best combination.</p>

<h2 id="choosing-the-components">Choosing the Components</h2>

<p>At this point I have to mention, that I’m looking for general purpose components in a Midi-Tower.</p>

<h3 id="case">Case</h3>

<p>Let’s go from the outer to the inner and start with the casing.</p>

<p>I don’t want to spend more than 100€ on the case, which should be a Midi-Tower (i.e, ATX) with
installed noise-insulation offering at least two USB 3.0 on the front panel and space for at least
four internal 3.25” drives.
In additional at least 280mm space for long graphics cards in default configuration <sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> is desirable.
Cable management and black colour would be nice.</p>

<p>A search on <a href="http://geizhals.eu/?cat=gehatx&amp;v=e&amp;hloc=de&amp;sort=artikel&amp;xf=533_ohne+Netzteil%7E534_ATX%7E540_USB+3.0%7E822_schwarz%7E535_schallged%E4mmt%7E536_4%7E551_140mm%7E4211_2%7E550_Midi-Tower%7E552_140mm"><em>Geizhals</em> revealed 39 different cases</a> from which two stood out after
sorting according to user and magazine reviews:
<a href="http://www.fractal-design.com/home/product/cases/define-series/define-r4-black-pearl"><em>Fractal Design Define R4 Black Pearl</em></a> and
<a href="http://www.cooltek.de/en/midi-tower/antiphon/12/antiphon"><em>Cooltek Antiphone black</em></a> (<a href="http://geizhals.eu/?cmp=888458&amp;cmp=812617">comparison</a>).</p>

<p>The <em>Cooltek Antiphone</em> is way smaller and less heavy, uses 120mm fans and has a true aluminium front.
320mm for graphics cards is sufficient for my use cases and 160mm space for the CPU cooler as well.</p>

<p>However, my winner is the <em>Fractal Design Define R4</em> despite its additional 4kg weight as it offers
two additional front USB interfaces (although only 2.0) and uses 140mm fans (which usually are a
little more quiet than 120mm).</p>

<h3 id="cpu">CPU</h3>

<p>Now, choosing the CPU will affect the selection of mainboards.
Call me bigoted, but I’m not going to look at <em>AMD</em> CPUs as I really want power efficient performance.</p>

<p>The tradeoff between performance and power consumption is probably best in the <em>Core i5</em> family.
Thus, going for the 5th generation (i.e. socket 1150) <em>Intel Core i5-4000</em> series seems a good choice.
But what model?</p>

<p>I’m sure I want a true quad core CPU with at most 65W TDP and boxed, although I’m not going to use
the stock cooler but the longer warranty is desirable.
This leaves us with <a href="http://geizhals.eu/?cat=cpu1150&amp;asuch=&amp;bpmax=&amp;v=e&amp;hloc=de&amp;plz=&amp;dist=&amp;sort=artikel&amp;xf=25_4~590_boxed~1133_Core+i5-4000~4_65">four models to choose</a>:
<em>i5-4440S</em>, <em>i5-4570S</em>, <em>i5-4590S</em> and <em>i5-4690S</em>.</p>

<p>At this point, it’s a close call between the newer <em>i5-4590S</em> and the older but minimal more
performant although 25€ more expensive <em>i5-4690S</em> (<a href="http://geizhals.eu/?cmp=1101777&amp;cmp=1101798">comparison</a>).</p>

<p>0.2 GHz for 25€? It’s a tough decision and both ways is fine.
I’m going for the <a href="http://ark.intel.com/de/products/80812/"><em>i5-4690S</em></a>.</p>

<h4 id="cooling">Cooling</h4>

<p>As mentioned, I’ll definitely not use the stock cooler from <em>Intel</em>.
There are more efficient and - more important - a lot more quiet solutions out there.</p>

<p>However, 65W TDP are not much and <a href="http://geizhals.eu/?cat=cpucooler&amp;asuch=&amp;bpmax=&amp;v=e&amp;hloc=at&amp;hloc=de&amp;hloc=pl&amp;hloc=uk&amp;hloc=eu&amp;plz=&amp;dist=&amp;mail=&amp;sort=p&amp;xf=817_1150%2F1155%2F1156~733_16"><em>Geizhals</em> lists 9 matching CPU coolers_</a> with
at most 16dB(A) noise.
With some additional reading of reviews, the <a href="http://www.alpenfoehn.de/index.php/en/cpu-cooler/silvretta"><em>EKL Alpenföhn Silvretta</em></a> seems a
really low priced but nevertheless absolutely sufficient choice.</p>

<p>I’ve read a lot about thermal compounds and how to apply these to finally choose the <a href="http://www.arctic.ac/de_en/products/cooling/thermal-compound/mx-2.html"><em>Arctic MX-2</em></a>
due to its easy of application at room temperatures and good heat conductivity.</p>

<h3 id="mainboard">Mainboard</h3>

<p>As we know the socket type we need, we can look for a matching mainboard.</p>

<p>My budget limit here is 80€.</p>

<p>The ATX board should have USB 3.0 (otherwise the requirement for the case would be pointless) and
PCIe 3.0 x16 for the graphics card as well as at least one additional PCIe slot for the TV card.
Regarding memory, four slots for DDR3-1600 are the way to go as I’m not going to do any overclocking.
Thus, an <em>Intel H97</em> chipset is sufficient (the other choice would be <em>Z97</em> allowing for overclocking).</p>

<p><a href="http://geizhals.eu/?cat=mbp4_1150&amp;asuch=&amp;bpmax=-80&amp;v=e&amp;hloc=at&amp;hloc=de&amp;hloc=pl&amp;hloc=uk&amp;hloc=eu&amp;plz=&amp;dist=&amp;mail=&amp;sort=artikel&amp;filter=sort&amp;xf=4400_1~3070_2~319_DDR3-1600~317_H97~2961_4~1244_6">With this selection <em>Geizhals</em> lists 3 boards</a>.
I don’t see any big differences and will go for the <a href="http://www.msi.com/product/mb/H97_PC_Mate.html"><em>MSI H97 PC-Mate</em></a>.</p>

<h3 id="graphics-card">Graphics Card</h3>

<p>Honestly, choosing the graphics card was the hardest part and took me the most time.</p>

<p>First, I thought I can set a budget limit of 150€, but after a day or two of searching and comparing
I decreased the limit to 100€.</p>

<p>I’m not favouring any vendor, neither <em>ATI</em> nor <em>nVidia</em>.
The card should definitely be a PCIe 3.0 x16 card and capable of running games release about two
years ago with full details in HD with a decent FPS (i.e. at least 30 in average), thus I’m good
advised to require at least 128bit memory bandwidth and GDDR5 memory.<br />
As I want full flexibility with regards to connecting monitors, at least one of each (HDMI, DVI and
DisplayPort) should be offered.</p>

<p><a href="http://geizhals.eu/?cat=gra16_512&amp;bpmax=-100&amp;v=e&amp;hloc=de&amp;filter=aktualisieren&amp;bl1_id=100&amp;sort=artikel&amp;xf=131_GDDR5~139_128~5424_1~5425_1~143_PCIe+3.0+x16~5426_1#xf_top">With this selection, <em>Geizhals</em> leaves me with 12 cards</a>.
And what do my eyes see there? A passively cooled card? And with good reviews?
<a href="http://www.sapphiretech.com/presentation/product/?cid=1&amp;gid=3&amp;sgid=1226&amp;pid=2130&amp;psn=&amp;lid=1&amp;leg=0"><em>Sapphire Radeon R7 250E Ultimate</em></a>, you’re my boy! 55W TDP!
As I’ve chosen a voluminous casing with good ventilation, cooling should not become a problem.</p>

<h3 id="memory">Memory</h3>

<p>Memory has really become cheap. Little more than 6€ per GB for kits with at least 8GB. Wow.</p>

<p>The only requirements I have are defined by the other components I’ve already chosen.
Thus, it should be DDR3-1600 DIMM modules in kits of two modules with at least 8GB in total.
I’m going for 2-module kits to leave space for additional two modules in the future.</p>

<p>With this, <a href="http://geizhals.eu/?cat=ramddr3&amp;asuch=&amp;bpmax=&amp;v=e&amp;hloc=de&amp;plz=&amp;dist=&amp;bl1_id=100&amp;sort=p&amp;xf=256_2x~253_8192~5828_DDR3~5015_1600~5831_DIMM~1454_4096"><em>Geizhals</em> knows of 149 kits</a> and sorting these by price lists a kit by
<em>Crucial</em> with a CL of 9 about same as other modules with a CL of 11 from other vendors.
In addition with numerous overrall positive reviews, I’m going for these:
<a href="http://www.crucial.com/usa/en/memory-ballistix-sport"><em>Crucial Ballistix Sports DIMM kit 8GB (2x 4GB)</em></a>.</p>

<h3 id="storage">Storage</h3>

<p>As mentioned before, I’m going for a SSD system drive and an additional HDD as data drive.</p>

<h4 id="ssd">SSD</h4>

<p>The system drive will only contain the operating systems (Linux and Windows) and few to no
additional software, thus 120GB should be sufficient.
As it’s a system drive, reading is the main task and that should be fast. A lower limit of 500MB/s
for the sequential read should do the job.</p>

<p>With the cheapest SSD found for 55€ and an upper bound of 70€ and additional constrains on the power
consumption (&lt;=0.5W idle and &lt;=2.5W load), I can choose between 10 cards
according to <a href="http://geizhals.eu/?cat=hdssd&amp;bpmax=-70&amp;v=e&amp;hloc=de&amp;bl1_id=100&amp;sort=p&amp;xf=252_120~221_500~2028_180~4830_1~2384_2.5~2385_0.5#xf_top"><em>Geizhals</em></a>.</p>

<p>The <a href="http://www.samsung.com/global/business/semiconductor/minisite/SSD/global/html/ssd850evo/overview.html"><em>Samsung SSDC850 EVO 120GB</em></a> is probably the best bet as well with regards to
random read and write.</p>

<h4 id="hdd">HDD</h4>

<p>For the data drive 1TB should be sufficient for the time being.
I have to admit, that I’m a little fan boy of <em>Western Digital</em> hard disc drives and never had any
failures over the last ten years and a dozen different drives.
Here, I’m looking for quiet and power efficiency drives with the usual desktop performance and thus
going for the <a href="http://www.wdc.com/de/products/products.aspx?id=780"><em>WD Green 1TB</em></a>.</p>

<h3 id="power-supply-unit-psu">Power Supply Unit (PSU)</h3>

<p>Up to now, the system has no power but a bunch of nice components.
Adding up all the maximum power consumptions of the various components gives us a total of about 200W.
To have a little head for additional drives or an active and more powerful graphics card, a PSU with
400W to 450W should be a good choice.</p>

<p>The casing can hold PSUs with up to 170mm depth at the dedicated bottom position.
With the additional constrain on the efficiency to “<em>80 PLUS Gold</em>” and ATX at least 2.31,
<a href="http://geizhals.eu/?cat=gehps&amp;v=e&amp;hloc=at&amp;hloc=de&amp;hloc=pl&amp;hloc=uk&amp;hloc=eu&amp;sort=p&amp;xf=360_400~3768_170~1248_450~4174_ATX~1119_4~4175_2.31#xf_top"><em>Geizhals</em> leaves us with 9 PSUs</a>.</p>

<p>I’m going for the <a href="http://geizhals.eu/?cat=gehps&amp;v=e&amp;hloc=at&amp;hloc=de&amp;hloc=pl&amp;hloc=uk&amp;hloc=eu&amp;sort=p&amp;xf=360_400~3768_170~1248_450~4174_ATX~1119_4~4175_2.31#xf_top"><em>be quiet! Straight Power 10 400W</em></a> due to their well received
support (5 years!) and reviews.</p>

<h3 id="tv-card">TV Card</h3>

<p>That was a short shot and I don’t know how good this will turn out.
I basically looked for a card with good Linux driver support.
Due to past experiences with the <em>Haupauge</em> card of the old system (with little to bad Linux support),
I’m now going for the <a href="http://www.tevii.com/Products_S472_1.asp"><em>TeVii S472</em></a>.</p>

<h2 id="summary">Summary</h2>

<p>All in all, the components of this system currently sum up to about 750€ plus shipping.
I’ve saved some money which I might spend on a entry-level steering wheel or game controller.</p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>This would actually be a really interesting project. However, <em>Geizhals</em> lists over 1200 PCIe
  GPUs, over 400 socket 1150 motherboards, over 1700 DDR3 RAM modules, little less than 100
  socket 1150 CPUs and over 1100 ATX PSUs. Already over 10<sup>13</sup> combinations alone for
  <em>Intel</em> socket 1150.
  Well, forget about that idea … <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Some cases offer the possibility to remove a secondary HDD frame to allow for longer graphics
  cards. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[C++ Code Coverage Analysis with CMake and Jenkins]]></title>
    <link href="http://torbjoern-klatt.de/article/2014/08/21/c++-code-coverage-analysis-with-cmake-and-jenkins/"/>
    <updated>2014-08-21T21:30:00+02:00</updated>
    <id>http://torbjoern-klatt.de/article/2014/08/21/c++-code-coverage-analysis-with-cmake-and-jenkins</id>
    <content type="html"><![CDATA[<p>Having a working test suite for your library or program is common knowledge.
Using a continuous integration workflow like <a href="">git-flow</a> backed by <a href="">Travis CI</a> or a <a href="">Jenkins</a>
instance is already a success story and widely used.
To ease up the build process of a C++ library/program on different platforms, many projects decide
to use <a href="">CMake</a>.</p>

<p>So far, so good.
But how good is your test suite?
Does it cover all the functionality and code in your library?
Does it catch all the different branches and edge-cases?</p>

<p>This little article describes a way of using <a href="">lcov</a> to generate a test coverage report for a
<em>CMake</em>-based C++ project.</p>

<!-- more -->

<p>Let us assume you have your C++ project set up with <em>CMake</em> as its build system.
As well, assume your code - including the test suite - compiles fine with <a href="">GCC</a>.
I probably don’t have to mention, that you will always want to do out-of-source builds.</p>

<h2 id="compiling-with-profiling-flags">Compiling with Profiling Flags</h2>

<p>The first step to enable coverage reports is to compile your program with <em>GCC</em>’s flags <code>-gp</code>, 
<code>-ftest-coverage</code> and <code>-fprofile-arcs</code> as well as linking them with <code>-fprofile-arcs</code>.
For documentation on those flags, see <a href="https://gcc.gnu.org/onlinedocs/gcc-4.8.3/gcc/Debugging-Options.html#Debugging-Options">GCC’s man page “3.9 Options for Debugging Your Program or GCC”</a>.</p>

<p>You can conditionally add those flags by defining a <em>CMake</em> option, e.g. <code>my_project_WITH_PROF</code>:</p>

<p><div class="highlight"><pre><code class="language-cmake" data-lang="cmake"><span class="nb">option</span><span class="p">(</span><span class="s">my_project_WITH_PROF</span> <span class="s">“Enable</span> <span class="s">profiling</span> <span class="s">and</span> <span class="s">coverage</span> <span class="s">report</span> <span class="s">analysis”</span> <span class="s">OFF</span><span class="p">)</span><span class="err">&lt;/p&gt;</span>

<span class="err">&lt;h1</span> <span class="err">id=&quot;section&quot;&gt;…&lt;/h1&gt;</span>

<span class="err">&lt;h1</span> <span class="err">id=&quot;assuming-target-myprog-exists&quot;&gt;assuming</span> <span class="err">target</span> <span class="err">“my_prog”</span> <span class="err">exists&lt;/h1&gt;</span>
<span class="err">&lt;p&gt;</span><span class="nb">if</span><span class="p">(</span><span class="o">${</span><span class="nv">CMAKE_COMPILER_ID</span><span class="o">}</span> <span class="s">MATCHES</span> <span class="s">GNU</span> <span class="s">AND</span> <span class="err">$</span><span class="s">my_project_WITH_PROF</span><span class="p">)</span>
    <span class="nb">set_target_properties</span><span class="p">(</span><span class="s">my_prog</span>
        <span class="s">PROPERTIES</span> <span class="s">COMPILE_FLAGS</span> <span class="s">“</span><span class="o">${</span><span class="nv">CMAKE_CXX_FLAGS</span><span class="o">}</span> <span class="s">-gp</span> <span class="s">-fprofile-coverage</span> <span class="s">-fprofile-args</span>
                   <span class="s">LINK_FLAGS</span> <span class="s">“-fprofile-arcs”</span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span></code></pre></div></p>

<p>That is all regarding compilation.
What is left is to run the compiled program, capture the coverage data and generate a HTML report out
of it.</p>

<h2 id="capturing-profiling-and-coverage-data">Capturing Profiling and Coverage Data</h2>

<p>For these steps you will need <a href="">lcov</a>, which brings in two commands: <code>lcov</code> and <code>genhtml</code>.</p>

<p>The procedure is simple as follows (in the same path as the executable):</p>

<ol>
  <li>
    <p>zero out preexisting coverage and profiling data</p>

    <pre><code>lcov --zerocounters  --directory .
</code></pre>
  </li>
  <li>
    <p>run the executable</p>

    <pre><code>./my_prog
</code></pre>
  </li>
  <li>
    <p>capture the coverage and profiling data</p>

    <pre><code>lcov --directory . --capture --output-file my_prog.info
</code></pre>
  </li>
  <li>
    <p>generate the HTML report</p>

    <pre><code>genhtml --output-directory coverage \
  --demangle-cpp --num-spaces 2 --sort \
  --title "My Program's Test Coverage" \
  --function-coverage --branch-coverage --legend \
  my_prog.info
</code></pre>
  </li>
</ol>

<h2 id="automation-with-cmake-and-ctest">Automation with CMake and CTest</h2>

<p>As stated in the beginning, we want coverage reports for our test suite, thus we will use <a href="">CTest</a>
for a nice and somehow “standardized” interface to our test suite (and because <em>CMake</em> comes with it).
We have enabled <em>CTest</em> in our <code>CMakeLists.txt</code> (call to <code>enable_testing()</code>) and added our <code>my_prog</code>
target as a test:</p>

<pre><code>add_test(NAME my_prog)
</code></pre>

<p>Now we can simply run <code>make test</code> or <code>ctest</code> in our build directory and it will not only run all
configured test, but also generate all the profiling and coverage data we later can capture with
<em>lcov</em> (cf. step 3 above).</p>

<p>But how can we zero out preexisting counters before running the test suite and gather all the 
different test’s data afterwards?</p>

<p>For this task, I have written a little shell script to do exactly that.
The script is meant to be run in the root folder of the project with the build directory as a child
of it.
As well, it assumes that the test suite is for a header-only library with the header files located
in <code>&lt;PROJECT_ROOT&gt;/include</code>.
To capture coverage of files in other directories you need to adjust the calls to <code>lcov --extract</code>
(line 40-41: adjust the glob pattern) and <code>genhtml</code> (line 50-60: adjust the <code>--prefix</code> argument).</p>

<p>For convenience, you might want to put your adjusted script in the root of your project and under
revision control.</p>

<p><div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#!/bin/sh</span>
<span class="nv">basepath</span><span class="o">=</span>&lt;code&gt;pwd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;function print_help <span class="o">{</span>
  <span class="nb">echo</span> “#######################################################################”
  <span class="nb">echo</span> “###               Generation of Test Coverage Report                <span class="c">###”</span>
  <span class="nb">echo</span> “#                                                                     <span class="c">#”</span>
  <span class="nb">echo</span> “# First <span class="o">(</span>and only<span class="o">)</span> parameter must be the name of the build directory  <span class="c">#”</span>
  <span class="nb">echo</span> “#                                                                     <span class="c">#”</span>
  <span class="nb">echo</span> “# Example:                                                            <span class="c">#”</span>
  <span class="nb">echo</span> “#   ./generate_coverage.sh build_gcc                                  <span class="c">#”</span>
  <span class="nb">echo</span> “#                                                                     <span class="c">#”</span>
  <span class="nb">echo</span> “#######################################################################”
  <span class="k">return</span> 0
<span class="o">}</span>&lt;/p&gt;

&lt;p&gt;if <span class="o">[[</span> <span class="nv">$# </span>-ne <span class="m">1</span> <span class="o">]]</span>
<span class="k">then</span>
  print_help
  <span class="nb">echo</span> “ERROR: Please name the build directory as the first parameter.”
  <span class="nb">exit</span> -1
<span class="k">fi</span>&lt;/p&gt;

&lt;p&gt;builddir<span class="o">=</span><span class="k">${</span><span class="nv">1</span><span class="k">}</span>
<span class="nb">cd</span> <span class="k">${</span><span class="nv">builddir</span><span class="k">}</span>&lt;/p&gt;

&lt;p&gt;rm -rf <span class="k">${</span><span class="nv">basepath</span><span class="k">}</span>/coverage
mkdir -p <span class="k">${</span><span class="nv">basepath</span><span class="k">}</span>/coverage&lt;/p&gt;

&lt;p&gt;for testdir in &lt;code&gt;find <span class="k">${</span><span class="nv">basepath</span><span class="k">}</span>/<span class="k">${</span><span class="nv">builddir</span><span class="k">}</span> -type d <span class="p">|</span> grep -o <span class="s1">&#39;tests/.*/.*\dir&#39;</span>&lt;/code&gt;
<span class="k">do</span>
  <span class="nv">testname</span><span class="o">=</span>&lt;code&gt;expr <span class="s2">&quot;$testdir&quot;</span> : <span class="s1">&#39;.*\(test_[a-zA-Z\-_]*\)\.dir&#39;</span>&lt;/code&gt;
  <span class="nb">echo</span> “Gathering Coverage <span class="k">for</span> <span class="k">${</span><span class="nv">testname</span><span class="k">}</span>”
  <span class="nb">cd</span> <span class="nv">$testdir</span>
  lcov –zerocounters  –directory .
  <span class="nb">cd</span> <span class="k">${</span><span class="nv">basepath</span><span class="k">}</span>/<span class="k">${</span><span class="nv">builddir</span><span class="k">}</span>
  ctest -R <span class="nv">$testname</span>
  <span class="nb">cd</span> <span class="nv">$testdir</span>
  lcov –directory . –capture –output-file <span class="k">${</span><span class="nv">testname</span><span class="k">}</span>.info.tmp
  lcov –extract <span class="k">${</span><span class="nv">testname</span><span class="k">}</span>.info.tmp “&lt;em&gt;<span class="k">${</span><span class="nv">basepath</span><span class="k">}</span>/include/**/&lt;/em&gt;” <span class="se">\</span>
    –output-file <span class="k">${</span><span class="nv">testname</span><span class="k">}</span>.info
  rm <span class="k">${</span><span class="nv">testname</span><span class="k">}</span>.info.tmp
  <span class="nb">cd</span> <span class="k">${</span><span class="nv">basepath</span><span class="k">}</span>/<span class="k">${</span><span class="nv">builddir</span><span class="k">}</span>
  <span class="k">if</span> <span class="o">[[</span> -e all_tests.info <span class="o">]]</span>
  <span class="k">then</span>
    lcov –add-tracefile all_tests.info <span class="se">\</span>
      –add-tracefile <span class="k">${</span><span class="nv">testdir</span><span class="k">}</span>/<span class="k">${</span><span class="nv">testname</span><span class="k">}</span>.info <span class="se">\</span>
      –output-file all_tests.info
  <span class="k">else</span>
    lcov –add-tracefile <span class="k">${</span><span class="nv">testdir</span><span class="k">}</span>/<span class="k">${</span><span class="nv">testname</span><span class="k">}</span>.info <span class="se">\</span>
      –output-file all_tests.info
  <span class="k">fi</span>
<span class="k">done</span>&lt;/p&gt;

&lt;p&gt;cd <span class="k">${</span><span class="nv">basepath</span><span class="k">}</span>
genhtml –output-directory ./coverage <span class="se">\</span>
  –demangle-cpp –num-spaces <span class="m">2</span> –sort <span class="se">\</span>
  –title “My Program Test Coverage” –prefix <span class="k">${</span><span class="nv">basepath</span><span class="k">}</span>/include <span class="se">\</span>
  –function-coverage –branch-coverage –legend <span class="se">\</span>
  <span class="k">${</span><span class="nv">basepath</span><span class="k">}</span>/<span class="k">${</span><span class="nv">builddir</span><span class="k">}</span>/all_tests.info</code></pre></div></p>

<h2 id="let-jenkins-publish-the-report">Let Jenkins Publish the Report</h2>

<p>For a real open source project a transparent integration process is vital, thus we want our <em>Jenkins</em>
instance to generate and publish the coverage reports for us.</p>

<p>In the - hopefully already existing - job where we run the test suite, simply add a step to execute
the above script.
Finally, on success, publish the result by adding a <em>Publish HTML reports</em> post-build action with
<code>coverage</code> as the HTML directory to archive, <code>index.html</code> as the index page and a good but short
name, e.g. <code>Test Coverage</code>.
After the first build, you will be able to browse the final HTML report at 
<code>&lt;your-jenkins-domain&gt;/job/&lt;MY_PROJECT_TESTS&gt;/Test_Coverage</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Struggling with Git Submodules and Mercurial Subrepositories]]></title>
    <link href="http://torbjoern-klatt.de/article/2013/04/03/struggling-with-git-submodules-and-mercurial-subrepositories/"/>
    <updated>2013-04-03T21:56:00+02:00</updated>
    <id>http://torbjoern-klatt.de/article/2013/04/03/struggling-with-git-submodules-and-mercurial-subrepositories</id>
    <content type="html"><![CDATA[<p>The popular and state-of-the-art <a href="https://en.wikipedia.org/wiki/Distributed_revision_control">distributed version control systems (DVCS)</a> 
tools <strong><a href="http://git-scm.com/">Git</a></strong> and <strong><a href="http://mercurial.selenic.com/">Mercurial</a></strong> 
are very likely the best way of reliably organising software development.
Both have numerous advantages over their predecessor <strong><a href="http://subversion.tigris.org/">Subversion (SVN)</a></strong>.</p>

<p>All three have a feature for handling nested repositories, which can but must not 
depend on each other: <em><a href="http://git-scm.com/docs/git-submodule">Git Submodules</a></em>, 
<em><a href="http://mercurial.selenic.com/wiki/Subrepository">Mercurial Subrepositories</a></em> 
and <em><a href="http://svnbook.red-bean.com/en/1.7/svn.advanced.externals.html">Subversion Externals</a></em>.</p>

<p>In this article I will describe an attempt of converting an existing SVN repository 
into a hierarchy of nested Git, Mercurial or SVN repositories while satisfying 
special demands on the whole setup.</p>

<p>If you are just interested in the final solution, scroll down to the conclusion.
But be warned …</p>

<!-- more -->

<h2 id="starting-point-and-current-setup">Starting Point and Current Setup</h2>

<h3 id="the-application">The Application</h3>
<p>The application called <em><a href="http://atlas.gcsc.uni-frankfurt.de/~ug/">ug4</a></em> is a 
scientific algorithm suite for mathematical modelling and simulation of various 
problems from basic diffusion to coupled systems in fluid dynamics.
It is written in C++ with heavy use of templated metaprogramming and an API 
enabling Lua-scripting and binding a Java-based GUI called <em><a href="http://vrl-studio.mihosoft.eu/">VRL-Studio</a></em>.
It compiles fluently on Unix, OSX and Windows and runs on pretty much all 
computers from Netbooks to super computers with thousands of cores as 
<a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JUQUEEN/JUQUEEN_node.html">JuQueen</a> 
or <a href="http://www.hlrs.de/?id=1546">HERMIT</a> with a remarkable performance.</p>

<h3 id="the-repository">The Repository</h3>
<p>The current SVN repository consists of a <strong>core</strong> application with three main 
and deeply interoperating <strong>libraries</strong> and some <em>glue-code</em>.
Beside this there are a few <em>core</em> and some <em>experimental</em> <strong>plugins</strong>.
Finally there are numerous <strong>apps</strong> using the shared library build from the core, 
the three libraries and selected plugins.</p>

<p>Third-party libraries are included by the core and main libraries and might be 
added by certain plugins.
For better usability and portability required third-party libraries are included 
as SVN Externals.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Directory structure of current repository </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/
</span><span class='line'>  apps/
</span><span class='line'>    app1/
</span><span class='line'>    app2/
</span><span class='line'>    secret-app/
</span><span class='line'>    …
</span><span class='line'>  docu/
</span><span class='line'>  core/
</span><span class='line'>    utils/
</span><span class='line'>    lib1/
</span><span class='line'>    lib2/
</span><span class='line'>    lib3/
</span><span class='line'>    some_further_folders/
</span><span class='line'>    some_files
</span><span class='line'>  plugins/
</span><span class='line'>    core/
</span><span class='line'>      core_plugin1/
</span><span class='line'>      core_plugin2/
</span><span class='line'>      …
</span><span class='line'>    experimental/
</span><span class='line'>      test_plugin/
</span><span class='line'>      broken_plugin/
</span><span class='line'>      secret_plugin/
</span><span class='line'>      …
</span><span class='line'>    some_files
</span><span class='line'>  unit_tests/
</span><span class='line'>    …
</span><span class='line'>  CMakeLists.txt
</span><span class='line'>  README
</span><span class='line'>  LICENSE
</span><span class='line'>  …</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Access rights are managed through a simple plaintext file setting branch and 
directory based read and write access on a per-user basis.
Though this is not as flexible as we wish, it is the safest way of ensuring none 
of the code based developed in collaboration with industrial partners leaks into 
other parts of the repository.</p>

<h4 id="the-group">The Group</h4>

<p>With just a very few exceptions all developers are mathematicians, physicists and 
computer scientists.
Only a few are trained and experienced in software development, while the rest 
is happy if a commit succeeds.
For most group members resolving conflicts is a nightmare and impossible task 
and will likely be circumvented or ignored due to laziness.
Adopting the basic SVN workflow of updating the working copy before making 
changes and committing those back is seen as an imposition by some group members.
One group member already asked for having an email notification through a 
post-commit hook on the server when his project partner committed changes, so he 
does not forget to pull in these changes.</p>

<h2 id="desired-end-point">Desired End Point</h2>

<h3 id="demands">Demands</h3>

<p>Each and every of those listed here are <em>show-stopping</em> requirements.
That means, if any of these is not fulfilled, the whole transition away from the
current setup will not happen.</p>

<h4 id="fine-grained-access-right-management">Fine-Grained Access Right Management</h4>

<p>Number one requirement is to reliably preserve and ensure read and write access 
for each and every group member and external collaborators.
This means, that there are some <em>secret</em> apps or plugins, which must only be 
readable by a few people.
External collaborators usually have only read access to the core part and maybe 
write access to one app or plugin (differs from person to person).
Temporary group members as students usually have write access to the core part 
and a few apps and plugins (differs from student to student) and read access to 
most other apps and plugins (also differs).</p>

<h4 id="global-changes-made-easy">(Global) Changes Made Easy</h4>

<p>Some of the core developers occasionally have to refactor and rename certain API 
functionality used by some apps and plugins.
They used to change and fix all apps using these functions in one go, i.e. in 
one commit for all apps.
Thus, recursive commits must be easily possible.</p>

<p>All commits and pushes of changes must be easily accomplished through 
<em><a href="http://www.eclipse.org/">Eclipse</a></em>, as it is widely used in our group.</p>

<h4 id="selective-checkouts">Selective Checkouts</h4>

<p>In principle most of the time all developers, collaborators and students only 
work on one app, maybe one or two plugins and rarely on the core and main libraries.
For 90% of the collaborators, changes made in other apps and plugins are of zero 
interest as those apps and plugins will only be used by very few people.
Thus it would be desirable not to have all apps and plugins checked out in the 
local working copy.</p>

<h4 id="easy-and-time-saving-maintenance">Easy and Time-Saving Maintenance</h4>

<p>It must be possible to manage and maintain the whole setup easily.
Thus, having a bunch of utility scripts just for handling the repository 
integrity is an absolute no-go.</p>

<h4 id="svn-mirror">SVN Mirror</h4>

<p>As some high-performance computers have extremely modified operating systems with 
dated tools installed, having only a Mercurial or Git server might result in 
serious trouble when working on such machines.
Subversion is always installed, though one will be lucky if the version reads 
1.5 or higher.</p>

<p>As well, for the sake of usability and adoption inside our group, during the 
transition phase the current SVN repository must still be writeable and two-way 
synchronised with the new repository setup.
A few month after the transition, the SVN server might become read-only to get 
rid of the frequent synchronisation and to force all developers to use the new 
repository.</p>

<h4 id="easy-branching-for-mini-group-collaboration">Easy Branching For Mini-Group Collaboration</h4>

<p>Due to the fact that some parts of the whole code base was developed in collaboration
with industrial partners and is subject to restrictive licenses and access rights,
SVN branches are deactivated for all developers.
With branches enabled it would be possible to accidentally merge and commit those
restricted code base into the main part, which would violate the licenses.</p>

<p>With separated repositories this branching for collaboratively experimenting with 
new features would be possible.</p>

<h3 id="wishes">Wishes</h3>

<p>These are <em>nice to have</em> and are <em>non-show-stopping</em>.
They can not compensate any of the aforementioned demands.</p>

<h4 id="automatic-checkouts-based-on-compile-parameters">Automatic Checkouts Based on Compile Parameters</h4>

<p>As we are already using <em><a href="http://www.cmake.org/">CMake</a></em> for managing the build 
process, with separated repositories for the different apps and plugins it would 
be desirable and nice to automate cloning and updating of the various repositories 
based on selected CMake parameters.
That means, if an app or plugin requires some other additional app or plugin, CMake
can check whether these dependencies are checked out and up to date.</p>

<h4 id="separated-version-histories">Separated Version Histories</h4>

<p>Up to now the revision history is a random mix of core, app and plugin changes.
To read and visualize only the history of, say, core additional filter commands
are required.
With separated repositories, the history will also be separated.</p>

<h2 id="git-submodules">Git Submodules</h2>

<h3 id="how-they-work">How they work</h3>

<p>Git Submodules are pointers on references of other Git repositories.
These Submodules are either managed by editing, adding and commiting a 
<code>.gitmodules</code> file in the root directory of the parent repository or via the 
subcommand <code>git submodule</code> (which edits and adds <code>.gitmodules</code> anyway).</p>

<h3 id="the-good-part">The Good Part</h3>

<p>With the help of <em><a href="https://github.com/sitaramc/gitolite">Gitolite</a></em> it is 
possible to easily manage and maintain all Git repositories and fine-grained user 
access rights.
As well, setting up SSH keys as the only authentication method is fast to accomplish
with Git and Gitolite.
This would be a lot more secure than the current setup using plaintext passwords.</p>

<p>Using feature branches to collaboratively experiment with new functionality is
one of the things Git is perfect at.</p>

<p>There are three experienced Git users in our group, which are happy to teach and 
train the rest of the group.</p>

<h3 id="gotchas">Gotchas</h3>

<p>At no point during committing and pushing the existence (from the point of view 
of the server) of referenced subrepositories is checked.
Thus, it is easily possible to commit a reference to a subrepository, which is only
a local repository.
When somebody else clones this repository from the server he will not be able to
check out the referenced subrepository and will end up in a detached head.
An absolute no-go!</p>

<p>Beside this, recursive adds, commits and pushes are off by default and have to be
enabled by a command line parameter on each execution (or by defining an alias).
As this requires client-side modifications and careful usage by all developers it 
is a no-go.</p>

<h3 id="when-they-work">When they work</h3>

<p>If all contributors are trained and experienced developers <em>and</em> advanced Git 
users.
If not, sooner than later all collaborators will end up with more detached heads 
and inconsistent states than they can think of.
And you have to fix them.</p>

<h2 id="mercurial-subrepositories">Mercurial Subrepositories</h2>

<h3 id="how-they-work-1">How they work</h3>

<p>As with Git Submodules, Mercurial Subrepositories are pointers to certain states 
of other Mercurial repositories.
These pointers are, similar to Git, managed by a dotfile (in this case <code>.hgsub</code>), 
which has to be under version control.</p>

<h3 id="the-good-part-1">The Good Part</h3>

<p>In contrast to Git, Mercurial always makes sure that all commits of subrepositories
referenced by the root repository are available on the server.
Thus, at no time it will be possible to push changes of the root repository without
also recursively pushing all changes of the subrepositories.</p>

<p>The most important Mercurial commands as <code>add</code>, <code>commit</code>, <code>update</code> and <code>status</code> 
have a simple flag (<code>-S</code>) to recursively trigger the same command in the 
subrepositories.
Thus, global changes in several apps and plugins can easily be committed by a single
command from the root repository.</p>

<p>With the extensions <em><a href="http://mercurial.selenic.com/wiki/HgSubversion">hgsubversion</a></em> 
and <em><a href="http://mercurial.selenic.com/wiki/ConvertExtension">convert</a></em> the conversion 
from SVN to Mercurial and splitting up the whole repository into the desired 
hierarchy of subrepositories worked smoothly.
Following the detailed instructions given in an <a href="https://blogs.atlassian.com/2011/03/goodbye_subversion_hello_mercurial/">article by Jason Hinch at Atlassian</a>
We could easily achieve the whole thing with a single Bash script, which is
available in <a href="https://gist.github.com/torbjoernk/5325343">this Gist</a>.
Be warned: depending on the size of the project it might take a couple of hours 
to run.</p>

<p>Similar to Gitolite for Git there is <em><a href="http://www.lshift.net/mercurial-server">mercurial-server</a></em> 
for Mercurial offering almost the same functionality of managing repositories 
and user access rights.</p>

<h3 id="gotchas-1">Gotchas</h3>

<p>There is no way of having Subrepositories without adding <em>and</em> committing the 
<code>.hgsub</code> file.
Thus, local-only subrepositories for the various apps and experimental plugins 
are impossible without any additional client-side scripts and hooks.
This is a no-go.</p>

<p>After conversion of the current SVN to a single Mercurial repository two-way 
synchronisation will work through post-commit hooks on both servers.
As soon as the Mercurial repository is split up into a hierarchy of subrepositories 
there will be no way of synchronising from SVN to Mercurial or vice-versa.
Thus, the transition will be a two-step process where the first is the transition 
to Mercurial leading to a complete shut-down of the SVN server and the second 
step will be the splitting into subrepositories.
While the latter is a one-time hard cut transition.
A no-go.</p>

<p>To hold it with two guys from Mercurial’s IRC channel on freenode:
<blockquote><p>nobody said subrepos made sense ;-)</p><footer><strong>kiilerix</strong> <cite>irc.freenode.net#mercurial on April 3rd 2013</cite></footer></blockquote>
<blockquote><p>subrepos are a Feature of Last Resort. By their very nature, they have to break <br/>a number of things.</p><footer><strong>JordiGH</strong> <cite>irc.freenode.net#mercurial on April 3rd 2013</cite></footer></blockquote>
I’m adding, that this also holds true for Git Submodules.</p>

<h3 id="when-they-work-1">When they work</h3>

<p>Mercurial subrepositories would be perfect for the core part with the main libraries 
and external dependencies as <em>Boost</em>.
The core part would always have a clean and consistent state and no undefined
references can occur.</p>

<h2 id="svn-externals">SVN Externals</h2>

<h3 id="how-they-work-2">How they work</h3>

<p>Similar to Git Submodules and Mercurial Subrepositories, SVN Externals are 
pointers to certain revisions of other SVN repositories.
They either can be a <em>floating</em> reference onto the <code>HEAD</code> revision or a fixed 
reference onto a specific revision, say <code>r42</code>.</p>

<h3 id="the-good-part-2">The Good Part</h3>

<p>Our current administrator does not need to learn any fancy new things.
Nor does the others of the group.</p>

<p>Access rights can still be managed not only by repository, branch and tag but also
by path, as Subversion allows partial commits and checkouts.</p>

<h3 id="gotchas-2">Gotchas</h3>

<p>Recursive commits out of the root repository into all subrepositories as provided 
by Mercurial, is partially implemented in the current development snapshot of 
version 1.8 (see <a href="http://subversion.tigris.org/issues/show_bug.cgi?id=1167#desc30">this comment on a Bug report</a>.
Without adding pre-commit hooks on the client side to automatically update the 
referenced revisions of the external subrepositories, it is hardly possible to 
achieve a clean state at all revisions of the root repository.
Again, client-side hooks and commit scripts are a no-go.</p>

<h3 id="when-they-work-2">When they work</h3>

<p>They are a great way of including certain releases or snapshots of third-party 
libraries into a project.
As we are doing it with <em><a href="http://glaros.dtc.umn.edu/gkhome/views/metis">METIS</a></em> 
or <em><a href="http://www.boost.org/">Boost</a></em>.
In case your developers are randomly changing code in the base repository and 
included ones, a lot of care has to be taken on each and every commit.
The must-have feature of easy usage is lost straight away.
Adding client side pre-commit hooks is far off being reliable nor easy maintainable.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you read that far and did not skip the previous three sections: Thank you for 
letting us share the pain with you, that obviously none of the popular version 
control systems are able to handle local-only subrepositories.
You might already guess our final decision.</p>

<p>Yes, we stick with the current setup and did not changed anything.</p>

<p>We only created a Gitolite server mirroring SVN <code>trunk</code> and SVN branches (both 
read-only) for those group members eager on using a full Git workflow.
An additional Git <em>“fetch repository”</em> gets noticed of new SVN commits by a 
post-commit hook on the SVN server which triggers a <code>git svn rebase</code> on <code>trunk</code> 
and the SVN branches and force-pushes those SVN changes to the Git mirror.
The Git mirror also allows all group members to easily create and mess around 
with feature and personal branches for the occasional <em>on-the-road-hack</em>.
Changes, which should go into the main development branch (i.e. <code>trunk</code>) must be 
committed directly to the SVN server via <code>git svn dcommit</code>.</p>

<p>This way, we still comply with the tight access rights on the code base and 
must-have comfort functions while having partial support of the agile workflows 
provided by Git’s features.
Everybody seems happy.
Except the guys, who stand this adventure.
They called in sick recovering from the pain endured that no usable version control system out 
there is capable of handling this use case in a maintainable manner.</p>

<p>Thanks to my colleagues and companions in this adventure, <a href="https://github.com/marscher">Martin Scherer</a> 
and <a href="https://github.com/stephanmg">Stephan Grein</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Problem with Apache2 and Subversion on openSUSE 11.4 solved]]></title>
    <link href="http://torbjoern-klatt.de/article/2011/06/08/problem-with-apache2-and-subversion-on-opensuse-11-4-solved/"/>
    <updated>2011-06-08T00:00:00+02:00</updated>
    <id>http://torbjoern-klatt.de/article/2011/06/08/problem-with-apache2-and-subversion-on-opensuse-11-4-solved</id>
    <content type="html"><![CDATA[<h3 id="preface">Preface</h3>
<p>This should not be a complete tutorial on how to set up a <em>Subversion</em> repository and <em>Apache2</em> on <em>openSUSE</em>, but should tackle a specific problem I had during configuration.</p>

<p>Here I describe my setup and configuration I’m running for having <code>httpd</code> and <code>svnserve</code> working in such a way, that I can browse the <em>Subversion</em> repositories via <code>http://</code> in a random webbrowser (either directly or in a more user-friendly way with <em>WebSVN</em>).
For security and privacy reasons only a small list of defined users can access (read and/or write) the webserver and repositories.</p>

<!-- more -->

<h3 id="background">Background</h3>
<p>After migrating my local workstation from <em>Ubuntu 10.04</em> to <em>openSUSE 11.4</em> I had to set up <em>Apache2</em> with <em>Subversion</em> (<code>svnserve</code>) again for my private repository.
Numerous tutorials are available on-line.
Some are specific for <em>openSUSE</em> as <a href="http://queens.db.toronto.edu/~nilesh/linux/subversion-howto/">this one</a>.
Some time ago I used <a href="http://alephzarro.com/blog/2007/01/07/installation-of-subversion-on-ubuntu-with-apache-ssl-and-basicauth">this one</a> for setting up everything under <em>Ubuntu</em>, which worked like charm.</p>

<p>However, following the above tutorial for <em>openSUSE</em> did not quite worked out for me and it took me quite a while to figure out the exact problem.</p>

<h3 id="sidemark">Sidemark</h3>
<p>I doubt the way I did it is the way everyone should do it.
For example, using the same user for <code>httpd</code> and <code>svnserve</code> is probably not the best solution.
However, I was not able to separate this yet and I am happy for any criticism, hints and suggestions.
In addition, it might be desirable using <code>https://</code> instead of <code>http://</code>.</p>

<h3 id="short">Short</h3>
<p>Having the SVN repository root in <code>/srv/svn</code>, the vhost root in <code>/srv/www/</code> and declaring an alias <code>/svn</code> pointing to <code>/srv/svn</code> (<code>Alias /svn "/srv/svn"</code>) in <code>/etc/apache2/conf.d/subversion.conf</code> is throwing up Apache’s DAV SVN module (as described <a href="http://www.rkrishardy.com/2009/12/subversion-fix-svn-copy-causes-repository-moved-permanentl/comment-page-1/#comment-8497">here</a>).</p>

<p>Browsing the repository in a random webbrowser is possible and does not throw any warnings or errors.
But when using <code>svn {import|checkout|commit}</code> the following error message is displayed</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>svn: Repository moved permanently to ‘http://svn.test.com/myproject/’; please relocate</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>The solution is moving the SVN repository root physically to <code>/srv/www/svn</code> and removing the Alias definition in <code>/etc/apache2/conf.d/subversion.conf</code>.</p>

<h3 id="detailed-configuration">Detailed Configuration</h3>
<p>####Subversion (svnserve)
Define <code>/srv/www/svn</code> as the Subversion root directory by editing <code>SVNSERVE_OPTIONS</code> in <code>/etc/sysconf/svnserve</code>.
In addition, <code>SVNSERVE_USERID</code> is changed to <code>wwwrun</code> and <code>SVNSERVE_GROUPID</code> to <code>www</code>.</p>

<p>Create the Subversion root directory <code>svn</code> in <code>/srv/www</code> and change the owner to <code>wwwrun:www</code>.
With the usual <code>svnadmin create myproject</code> you create the repositories.</p>

<p>Finally make sure the user <code>wwwrun</code> has read, write and execute rights for the complete <code>/srv/www/svn</code> path.
If it has not for <code>/srv</code> but for <code>/srv/www</code> it will very likely not work.</p>

<h4 id="apache2-httpd">Apache2 (httpd)</h4>
<p>After installation through <em>YAST</em>, there is a template config file in <code>/etc/apache2/conf.d</code> called <code>subversion.conf</code>.
Make sure the complete <code>mod_alias.c</code> block is commented out.
In my case I also left the <code>/srv/svn/html</code> block commented out.</p>

<p>For each project a separate <code>Location</code> block needs to be defined.
In my case I only want the users <code>user1</code> and <code>user2</code> having access to the repository — no one else.
These users are defined in <code>/etc/apache2/users.cred</code> with the standard <code>htpasswd2</code> method.
Thus, the <code>Location</code> block looks like this:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;Location /svn/myproject&gt;
</span><span class='line'>  DAV svn
</span><span class='line'>  SVNPath /srv/www/svn/myproject
</span><span class='line'>  Order allow,deny
</span><span class='line'>  Allow from all
</span><span class='line'>  AuthType Basic
</span><span class='line'>  AuthName “Project 1”
</span><span class='line'>  AuthUserFile /etc/apache2/users.cred
</span><span class='line'>  Require user user1 user2
</span><span class='line'>&lt;/Location&gt;</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>As usual you also can define user groups in <code>/etc/apache2/groups.cred</code> and use <code>AuthGroupFile /etc/apache2/groups.cred</code> and <code>Require group</code> — or any other authorisation method.</p>

<p>Make sure, <code>mod_dav</code> and <code>mod_dav_svn</code> are loaded.
One way is to append <code>dav</code> and <code>dav_svn</code> to the line <code>APACHE_MODULES</code> in <code>/etc/sysconf/apache2</code>.</p>

<h4 id="websvn">WebSVN</h4>
<p>After installation through YAST use <code>/etc/websvn/config.php</code> for configuration.
This file should be self-explaining.</p>

<p>Finally restart <code>svnserve</code> and <code>Apache2</code> and everything should be fine.</p>

<p><em>Remark: This text has been <a href="http://forums.opensuse.org/english/get-technical-help-here/how-faq-forums/unreviewed-how-faq/461165-howto-apache2-subversion-svn-access-control-opensuse-11-4-a.html">cross-posted to the openSUSE Forums</a> on June 8th, 2011</em></p>
]]></content>
  </entry>
  
</feed>
